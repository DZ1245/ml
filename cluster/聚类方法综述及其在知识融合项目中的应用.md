

[TOC]

# 一、什么是聚类

## 1.1 聚类的定义

`聚类(Clustering)`是按照某个特定标准(如距离)把一个数据集分割成不同的类或簇，使得同一个簇内的数据对象的相似性尽可能大，同时不在同一个簇中的数据对象的差异性也尽可能地大。也即聚类后同一类的数据尽可能聚集到一起，不同类数据尽量分离。

## 1.2 聚类和分类的区别

+ `聚类(Clustering)`：是指把相似的数据划分到一起，具体划分的时候并不关心这一类的标签，目标就是把相似的数据聚合到一起，聚类是一种`无监督学习(Unsupervised Learning)`方法。
+ `分类(Classification)`：是需要标注数据是某种具体的类型，通过训练数据集获得一个分类器，再通过分类器去预测未知数据的过程，分类是一种`监督学习(Supervised Learning)`方法。

## 1.3 聚类的一般过程

1. 数据准备：特征标准化和降维
2. 特征选择：从最初的特征中选择最有效的特征，并将其存储在向量中
3. 特征提取：通过对选择的特征进行转换形成新的突出特征
4. 聚类：基于某种距离函数进行相似度度量，获取簇
5. 聚类结果评估：分析聚类结果，如`SSE`等

## 1.4 数据对象间的相似度度量

对于数值型数据，可以使用下表中的相似度度量方法。

| **相似度度量准则** | **相似度度量函数** |
| :------------: | :------------: |
| Euclidean 距离 | $d( \boldsymbol{x}, \boldsymbol{y})=\sqrt{\sum_{i=1}^{n}(x_i-y_i)^2}$ |
| Manhattan 距离 | $d( \boldsymbol{x}, \boldsymbol{y})=\sum_{i=1}^{n}\left |x_i-y_i \right|$ |
| Chebyshev 距离 | $d( \boldsymbol{x}, \boldsymbol{y})=\max_{i=1,2,…,n}^{n}\left|x_i-y_i \right|$ |
| Minkowski 距离 | $d( \boldsymbol{x}, \boldsymbol{y})=[\sum_{i=1}^{n}(x_i-y_i)^p]^ {\frac{1}{p}}$ |

`Minkowski `距离就是$ Lp $范数（$p≥1$)，而 `Manhattan` 距离、`Euclidean `距离、`Chebyshev `距离分别对应 $p=1,2,∞ $时的情形。

## 1.5 cluster之间的相似度度量

除了需要衡量对象之间的距离之外，有些聚类算法（如层次聚类）还需要衡量`cluster`之间的距离 ，假设$ C_i $和$ C_j$ 为两个 `cluster`，则前四种方法定义的 $C_i $和 $C_j$ 之间的距离如下表所示：

| **相似度度量准则** | **相似度度量函数** |
| :------------: | :------------: |
| Single-link | $D(C_i,C_j)= \min_{\boldsymbol{x}\subseteq C_i, \boldsymbol{y}\subseteq C_j}d( \boldsymbol{x}, \boldsymbol{y})$ |
| Complete-link | $D(C_i,C_j)= \max_{\boldsymbol{x}\subseteq C_i, \boldsymbol{y}\subseteq C_j}d( \boldsymbol{x}, \boldsymbol{y})$ |
| UPGMA | $D(C_i,C_j)= \frac{1}{\left | C_i\right |\left | C_j\right |}\sum_{\boldsymbol{x}\subseteq C_i, \boldsymbol{y}\subseteq C_j}d( \boldsymbol{x}, \boldsymbol{y})$ |
| WPGMA | - |

其中，`Single-link`定义两个`cluster`之间的距离为两个`cluster`之间距离最近的两个点之间的距离，这种方法会在聚类的过程中产生`链式效应`，即有可能会出现非常大的`cluster`，而`Complete-link`定义的是两个`cluster`之间的距离为两个``cluster`之间距离最远的两个点之间的距离，这种方法可以避免`链式效应`,对异常样本点（不符合数据集的整体分布的噪声点）却非常敏感，容易产生不合理的聚类，`UPGMA`正好是`Single-link`和`Complete-link`方法的折中，他定义两个`cluster`之间的距离为两个`cluster`之间所有点距离的平均值，最后一种`WPGMA`方法计算的是两个 `cluster` 之间两个对象之间的距离的加权平均值，加权的目的是为了使两个 `cluster` 对距离的计算的影响在同一层次上，而不受 `cluster` 大小的影响，具体公式和采用的权重方案有关。

# 二、数据聚类方法

数据聚类方法主要可以分为`划分式聚类方法(Partition-based Methods)`、`基于密度的聚类方法(Density-based methods)`、`层次化聚类方法(Hierarchical Methods)`等。

![image-20191030103040431](https://tva1.sinaimg.cn/large/006y8mN6ly1g8g0rfx2wrj30uu0ho75l.jpg)

## 2.1 划分式聚类方法

 划分式聚类方法需要事先指定簇类的数目或者聚类中心，通过反复迭代，直至最后达到<font color=red>"簇内的点足够近，簇间的点足够远"</font>的目标。经典的划分式聚类方法有`k-means`及其变体`k-means`、`k-medians`、`kernel k-means`。

### 2.1.2 k_means算法

经典的`k-means`算法的流程如下：

>1. 创建$k$个点作为初始质心(通常是随机选择)
>2. 当任意一个点的簇分配结果发生改变时
>    1. 对数据集中的每个数据点
>        1. 对每个质心
>            1. 计算质心与数据点之间的距离
>        2. 将数据点分配到距其最近的簇
>    2. 对每个簇，计算簇中所有点的均值并将均值作为质心

经典`k-means`源代码:

原始数据集通过观察发现大致可以分为4类，所以取$k=4$，测试数据效果如下图所示。

![kmeans](https://tva1.sinaimg.cn/large/006y8mN6ly1g8ggk4l54pj31uo0qowh4.jpg)

看起来很顺利，但事情并非如此，我们考虑k-means算法中最核心的部分，假设$x_i(i=1,2,…,n)$是数据点，$\mu_j(j=1,2,…,k)$是初始化的数据中心，那么我们的目标函数可以写成
$$
\min\sum_{i=1}^{n} \min \limits_{j=1,2,...,k}\left |\left |  x_i -\mu_j\right | \right |^2
$$
这个函数是非凸优化函数，会收敛于局部最优解，可以参考[证明过程](https://math.stackexchange.com/questions/463453/how-to-see-that-k-means-objective-is-convex)。举个🌰，$\mu_1=\left [ 1,1\right ] ,\mu_2=\left [ -1,-1\right ]$，则
$$
z=\min \limits_{j=1,2}\left |\left |  x_i -\mu_j\right | \right |^2
$$
该函数的曲线如下图所示

![局部最优](https://tva1.sinaimg.cn/large/006y8mN6ly1g8g0fi2iidj30zk0qo7ek.jpg)
可以发现该函数有两个局部最优点，当时初始质心点取值不同的时候，最终的聚类效果也不一样，接下来我们看一个具体的实例。

![划分错误](https://tva1.sinaimg.cn/large/006y8mN6ly1g8ggkzj493j31uo0qoq4z.jpg)

在这个例子当中，下方的数据应该归为一类，而上方的数据应该归为两类，这是由于初始质心点选取的不合理造成的误分。而$k$值的选取对结果的影响也非常大，同样取上图中数据集，取$k=2,3,4$，可以得到下面的聚类结果：

![k值不同](https://tva1.sinaimg.cn/large/006y8mN6ly1g8h5cyijfuj31uo0hsq57.jpg)

一般来说，经典k-means算法有以下几个特点：

1. 需要提前确定$k$值
2. 对初始质心点敏感
3. 对异常数据敏感

### 2.1.2 kpp_means算法

`k-means++`是针对`k-means`中初始质心点选取的优化算法。该算法的流程和`k-means`类似，改变的地方只有初始质心的选取，该部分的算法流程如下

> 1. 随机选取一个数据点作为初始的聚类中心
> 2. 当聚类中心数量小于$k$
>     1. 计算每个数据点与当前已有聚类中心的最短距离，用$D(x)$表示，这个值越大，表示被选取为下一个聚类中心的概率越大，最后使用轮盘法选取下一个聚类中心

使用`k-means++`对上述数据做聚类处理，得到的结果如下

![kpp_means](https://tva1.sinaimg.cn/large/006y8mN6ly1g8i9k80yd1j31uo0qo40n.jpg)

### 2.1.3 bi_kmeans算法

一种度量聚类效果的指标是`SSE(Sum of Squared Error)`，他表示聚类后的簇离该簇的聚类中心的平方和，`SSE`越小，表示聚类效果越好。 `bi_kmeans`是针对`kmeans`算法会陷入局部最优的缺陷进行的改进算法。该算法基于SSE最小化的原理，首先将所有的数据点视为一个簇，然后将该簇一分为二，之后选择其中一个簇继续进行划分，选择哪一个簇进行划分取决于对其划分是否能最大程度的降低`SSE`的值。

该算法的流程如下：

> 1. 将所有点视为一个簇
> 2. 放簇的个数小于$k$时
>     1. 对每一个簇
>         1. 计算总误差
>         2. 在给定的簇上面进行`kmeans`聚类($k=2$)
>         3. 计算将该簇一分为二之后的总误差
>     2. 选取使得误差最小的那个簇进行划分操作

## 2.2 基于密度的方法

## 2.3 层次化聚类方法

![hierarchical_clustering](https://tva1.sinaimg.cn/large/006y8mN6ly1g8ipc9dng5j310k0pgwgp.jpg)

## 2.4 聚类方法比较

# 三、分布式聚类配置方法

# 四、小区融合项目应用

## 4.1 聚类方法的选取

## 4.2 聚类效果

# 五、参考文献

[1] 李航.统计学习方法

[2] Peter Harrington.Machine Learning in Action/李锐.机器学习实战

[3] https://www.zhihu.com/question/34554321

[4] [T. Soni Madhulatha.AN OVERVIEW ON CLUSTERING METHODS](https://arxiv.org/pdf/1205.1117.pdf)

[5] https://zhuanlan.zhihu.com/p/32375430

[6] [http://heathcliff.me/聚类分析（一）：层次聚类算法](http://heathcliff.me/聚类分析（一）：层次聚类算法/)